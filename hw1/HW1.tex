\documentclass[12pt,letterpaper]{hmcpset}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{enumitem}

\input{macros.tex}

% info for header block in upper right hand corner
\name{}
\class{Math189R SP19}
\assignment{Homework 1}
\duedate{Monday, Feb 4, 2017}

\renewcommand{\labelenumi}{{(\alph{enumi})}}


\begin{document}
Feel free to work with other students, but make sure you write up the homework
and code on your own (no copying homework \textit{or} code; no pair programming).
Feel free to ask students or instructors for help debugging code or whatever else,
though.
The starter code for problem 2 part c and d can be found under the Resource tab on course website.\\

\textit{Note:} You need to create a Github account for submission of the coding part of the homework. Please create a repository on Github to hold all your code and include your Github account username as part of the answer to problem 2.

\begin{problem}[1]
(\textbf{Linear Transformation}) Let $\mathbf{y} = A\mathbf{x} + \mathbf{b}$ be a random vector.
show that expectation is linear:
\[
    \EE[\yy] = \EE[A\xx + \bb] = A\EE[\xx] + \bb.
\]
Also show that
\[
    \cov[\yy] = \cov[A\xx + \bb] = A \cov[\xx] A^\T = A\Sigmab A^\T.
\]
\end{problem}
\begin{solution} 

	We wish to show that \EE[\yy] = \EE[A\xx + \bb] = A\EE[\xx] + \bb. By definition of expectation, we have $$\EE[\yy] = \int_S(A\xx + \bb)\PP(x)dx,$$ where $S$ is the space contains all possible $x$. Then, we can rearrange to get $$\EE[\yy] =  A\int_S\xx \PP(x)dx +  \bb\int_S\PP(x)dx.$$ Note that by definition of expectation, $\EE[\xx] =\int_S\xx \PP(x)dx$. Also by definition of random variables $\int_S\PP(x)dx =1$. Thus, we can see that $\EE[\yy] =  A\EE[\xx] +  \bb.$\\
    	
    Next, we wish to show that $\cov[\yy] = \cov[A\xx + \bb] = A \cov[\xx] A^\T = A\Sigmab A^\T.$ We are given $\yy = A\xx + \bb$ so that $\cov[\yy] = \cov[A\xx+\bb]$. Then, by definition of covariance, we have $\cov[A\xx+\bb] = \EE[(A\xx+b - \EE[A\xx+b])(A\xx+b - \EE[A\xx+b])^T]$. By linearity of expectation, we can rewrite as $\cov[A\xx+\bb] = \EE[(A\xx+b - A\EE[\xx] - b)(A\xx+b - A\EE[\xx]-b)^T]$. Simplifying, we get $\cov[A\xx+\bb] = \EE[(A\xx - A\EE[\xx])(A(\xx- \EE[\xx]))^T]$. Applying transpose in the second term inside expectation, we get $\cov[A\xx+\bb] = \EE[(A\xx - A\EE[\xx])(\xx- \EE[\xx])^TA^T]$. Since $A$ and $A^T$ are constants, by linearity, we have $\cov[A\xx+\bb] = A\EE[(\xx - A\EE[\xx])(\xx- \EE[\xx])^T]A^T$. Lastly, by definition of covariance, we see $\cov[A\xx+\bb] = A\cov[\xx]A^T = A\Sigmab A^T$ as desired.
\end{solution}
\newpage


\begin{problem}[2]
Given the dataset $\Dc = \{(x,y)\} = \{(0,1), (2,3), (3,6), (4,8)\}$
\begin{enumerate}
   \item Find the least squares estimate $y = \thetab^\T\xx$ by hand using
        Cramer's Rule.
    \item Use the normal equations to find the same solution and verify it
        is the same as part (a).
    \item Plot the data and the optimal linear fit you found.
    \item Find randomly generate 100 points near the line with white Gaussian
        noise and then compute the least squares estimate (using a computer).
        Verify that this new line is close to the original and plot the new
        dataset, the old line, and the new line.
\end{enumerate}

\end{problem}
\begin{solution}
    \begin{enumerate}
    	\item Let $$X = \begin{bmatrix}
1 & 0 \\
1 & 2 \\
1 & 3 \\
1 & 4
\end{bmatrix}\text{ and } \yy = \begin{bmatrix}
1 \\
3 \\
6 \\
8
\end{bmatrix}.$$
To find the least square estimate using Cramer's Rule, we need to set $X^TX \theta = X^T\yy$ where $\theta = \begin{bmatrix}
	\theta_0\\
	\theta_1
\end{bmatrix}$. Plugging in, we get $$X^TX\theta = \begin{bmatrix}
	4 & 9\\
	9 & 29
\end{bmatrix}\begin{bmatrix}
	\theta_0\\
	\theta_1
\end{bmatrix} = X^T\yy = \begin{bmatrix}
	18\\
	56
\end{bmatrix}.$$
Next, we apply Cramer's Rule to solve for $\theta_0$ and $\theta_1$. $$\theta_0 =\frac{
\begin{vmatrix}
18 & 9 \\
56 & 29
\end{vmatrix}
}{
\begin{vmatrix}
4 & 9 \\
9 & 29
\end{vmatrix}
} = \frac{18}{35} \text{ and }\theta_1 =\frac{
\begin{vmatrix}
4 & 18 \\
9 & 56
\end{vmatrix}
}{
\begin{vmatrix}4 & 9 \\
9 & 29
\end{vmatrix}
} = \frac{62}{35}$$

Thus, for the given dataset,  we found the least square estimate is $y = \frac{18}{35} +\frac{62}{35}x$ using Cramer's Rule

\newpage
\item Using the normal equation we can solve for $\theta$ using the following formula,
$$\theta = (X^TX)^{-1}X^T\yy.$$ Then, we can plug in values from part a and evaluate, $$\theta = \begin{bmatrix}
	4 & 9\\
	9 & 29
\end{bmatrix}^{-1} \begin{bmatrix}
	1 & 1 & 1 & 1\\
	0 & 2 & 3 & 4
\end{bmatrix}\begin{bmatrix}
1 \\
3 \\
6 \\
8
\end{bmatrix} = \begin{bmatrix}
\frac{18}{35}\\
\frac{62}{35}
\end{bmatrix}.$$
Thus, we see that the solution using the normal equation is the same as using Cramer's rule as in part a.

\item Plotted data and optimal linear fit.

\includegraphics{hw1pr2c.png}
\newpage
\item Plotted data with Gaussian noise and new least square estimate. Note that the new line is close to the old line.

\includegraphics{hw1pr2d.png}
    \end{enumerate}


\end{solution}
\newpage



\end{document}