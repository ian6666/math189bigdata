\documentclass[12pt,letterpaper,fleqn]{hmcpset}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{parskip}

\input{macros.tex}

% info for header block in upper right hand corner
\name{}
\class{Math189R SP19}
\assignment{Homework 5}
\duedate{Monday, Mar 11, 2019}

\begin{document}

Feel free to work with other students, but make sure you write up the homework
and code on your own (no copying homework \textit{or} code; no pair programming).
Feel free to ask students or instructors for help debugging code or whatever else,
though.

\begin{problem}[1]
\textbf{(Murphy 12.5 - Deriving the Residual Error for PCA)} It may be helpful to reference
section 12.2.2 of Murphy.
\begin{enumerate}[(a)]
    \item Prove that
        \[
            \left\|\xx_i - \sum_{j=1}^k z_{ij}\vv_j\right\|^2 = \xx_i^\T\xx_i - \sum_{j=1}^k\vv_j^\T \xx_i\xx_i^\T \vv_j.
        \]
        Hint: first consider the case when $k=2$. Use the fact that $\vv_i^\T\vv_j$ is 1 if $i=j$ and 0 otherwise.
        Recall that $z_{ij} = \xx_i^\T\vv_j$.

    \item Now show that
        \[
            J_k = \frac{1}{n}\sum_{i=1}^n \left(\xx_i^\T \xx_i - \sum_{j=1}^k\vv_j^\T \xx_i\xx_i^\T \vv_j\right) = \frac{1}{n}\sum_{i=1}^n \xx_i^\T\xx_i - \sum_{j=1}^k\lambda_j.
        \]
        Hint: recall that $\vv_j^\T \Sigmab \vv_j = \lambda_j\vv_j^\T\vv_j = \lambda_j$.

    \item If $k=d$ there is no truncation, so $J_d=0$. Use this to show that the error from only using $k<d$
        terms is given by
        \[
            J_k = \sum_{j=k+1}^d \lambda_j.
        \]
        Hint: partition the sum $\sum_{j=1}^d \lambda_j$ into $\sum_{j=1}^k \lambda_j$ and $\sum_{j=k+1}^d \lambda_j$.
\end{enumerate}
\end{problem}
\begin{solution}
\begin{enumerate}[(a)] 
    \item 
    \begin{align*}
        \left\| x_i - \sum_{j=1}^k z_{ij}v_j \right\|^2 &= \left( x_i - \sum_{j=1}^k z_{ij}v_j \right)^\top \left( x_i - \sum_{j=1}^k z_{ij}v_j \right) \\
        &= x_i^\top x_i - 2\sum_{j=1}^k z_{ij}v_j^\top x_i + \left( \sum_{j=1}^k z_{ij}v_j \right)^\top \left( \sum_{j=1}^k z_{ij}v_j \right) \\
        &= x_i^\top x_i - 2\sum_{j=1}^k z_{ij}v_j^\top x_i + \sum_{j=1}^k \sum_{l=1}^k z_{ij}v_j^\top z_{il}v_l \\
        &= x_i^\top x_i - 2\sum_{j=1}^k z_{ij}v_j^\top x_i + \sum_{j=1}^k v_j^\top x_i x_i^\top v_j \\
        &= x_i^\top x_i - 2\sum_{j=1}^k z_{ij}v_j^\top x_i + \sum_{j=1}^k v_j^\top x_i x_i^\top v_j \quad (\text{since } v_j^\top v_i = 1 \text{ if } i = j) \\
        &= x_i^\top x_i - \sum_{j=1}^k z_{ij}v_j^\top x_i v_j^\top,
    \end{align*} 

    \item By definition
    \begin{align*}
        J_k &= \frac{1}{n} \sum_{i=1}^n \left( x_i^\top x_i - \sum_{j=1}^k z_{ij}v_j^\top x_i x_i^\top v_j \right) \\
        &= \frac{1}{n} \sum_{i=1}^n \left( x_i^\top x_i - \sum_{j=1}^k v_j^\top \frac{1}{n} \left( \sum_{i=1}^n x_i x_i^\top \right) v_j \right) \\
        &= \frac{1}{n} \sum_{i=1}^n x_i^\top x_i - \sum_{j=1}^k v_j^\top \Sigma v_j \\
        &= \frac{1}{n} \sum_{i=1}^n x_i^\top x_i - \sum_{j=1}^k \lambda_j,
    \end{align*}
    \item Since $J_d = 0$, $\sum_{j=1}^d \lambda_j = \frac{1}{n}\sum_{i=1}^n x_i^\top x_i$. Then
\begin{align*}
    J_k &= \frac{1}{n} \sum_{i=1}^n x_i^\top x_i - \sum_{j=1}^d \lambda_j + \sum_{j=k+1}^d \lambda_j \\
    &= \sum_{j=k+1}^d \lambda_j.
\end{align*}
\end{enumerate}
\end{solution}
\newpage



\begin{problem}[2]
\textbf{($\ell_1$-Regularization)} Consider the $\ell_1$ norm of a vector $\xx\in\RR^n$:
\[
    \|\xx\|_1 = \sum_i |\xx_i|.
\]
Draw the norm-ball $B_k = \{\xx : \|\xx\|_1 \leq k\}$ for $k=1$. On the same graph, draw the Euclidean norm-ball $A_k = \{\xx : \|\xx\|_2 \leq k\}$ for $k=1$ behind the first plot. (Do not need to write any code, draw the graph by hand).
\newline
\newline
Show that the optimization problem
\begin{align*}
    \text{minimize: } & f(\xx)\\
    \text{subj. to: } & \|\xx\|_p \leq k
\end{align*}
is equivalent to
\begin{align*}
    \text{minimize: } & f(\xx) + \lambda\|\xx\|_p
\end{align*}

(hint: create the Lagrangian). With this knowledge, and the plots given above, argue why
using $\ell_1$ regularization (adding a $\lambda\|\xx\|_1$ term to the objective) will give
sparser solutions than using $\ell_2$ regularization for suitably large $\lambda$.
\end{problem}
\begin{solution}
Drawing of the balls $B_k$ and $A_k$:
\begin{center}
	\includegraphics[scale=0.2]{ball.png}
\end{center}

We approach the optimization problem with the goal to minimize $f(x)$ subject to the constraint $\|x\|_p \leq k$. This is equivalent to the problem of finding the infimum over $x$ and the supremum over $\lambda \geq 0$ of the Lagrangian $L(x, \lambda) = f(x) + \lambda(\|x\|_p - k)$.

The dual form allows us to exchange the infimum and supremum, expressed as:
\[
\sup_{\lambda \geq 0} \inf_x \{ f(x) + \lambda(\|x\|_p - k) \} = \sup_{\lambda \geq 0} g(\lambda)
\]

The value of $x$ that minimizes $f(x) + \lambda(\|x\|_p - k)$ will also be the minimizer for $f(x) + \lambda\|x\|_p$ since the term $-\lambda k$ is independent of $x$. Therefore, the optimization can be simplified to:
\[
\text{minimize} \{ f(x) + \lambda\|x\|_p \}
\]
for an appropriate $\lambda \geq 0$.

Considering this in the context of $\ell_1$ regularization, we interpret it as projecting the true optimal solution of the problem onto an $\ell_1$ norm ball. The geometry of the $\ell_1$ norm ball, characterized by its sharper vertices, increases the likelihood of the solution having elements that are exactly zero, unlike the $\ell_2$ norm ball which is rotationally invariant. In higher dimensions, the $\ell_1$ penalty thus favors solutions with more zero weights in comparison to the $\ell_2$ penalty, achieving the desired sparsity.

\end{solution}
\newpage



\begin{problem}[Extra Credit]
\textbf{(Lasso)} Show that placing an equal zero-mean Laplace prior on each element of the weights $\thetab$
of a model is equivelent to $\ell_1$ regularization in the Maximum-a-Posteriori estimate
\begin{align*}
    \text{maximize: } & \PP(\thetab | \Dc) = \frac{\PP(\Dc | \thetab)\PP(\thetab)}{\PP(\Dc)}.
\end{align*}
Note the form of the Laplace distribution is
\[
    \mathrm{Lap}(x|\mu,b) = \frac{1}{2b}\exp\left(-\frac{|x-\mu|}{b}\right)
\]
where $\mu$ is the location parameter and $b>0$ controls the variance. Draw (by hand) and compare the density
$\mathrm{Lap}(x|0,1)$ and the standard normal $\Nc(x|0,1)$ and suggest why this would
lead to sparser solutions than a Gaussian prior on each elements of the weights
(which correspond to $\ell_2$ regularization).
\end{problem}
\begin{solution}
\vfill
\end{solution}

\end{document}
