\documentclass[12pt,letterpaper,fleqn]{hmcpset}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{parskip}

\input{macros.tex}

% info for header block in upper right hand corner
\name{}
\class{Math189R SP19}
\assignment{Homework 7}
\duedate{Wednesday, Apr 8, 2019}

\begin{document}

Feel free to work with other students, but make sure you write up the homework
and code on your own (no copying homework \textit{or} code; no pair programming).
Feel free to ask students or instructors for help debugging code or whatever else,
though.
\newline
\newline
The starter files for problem 2 can be found under the Resource tab on course website. The plot for problem 2 generated by the sample solution has been included in the starter files for reference. Please print out all the graphs generated by your own code and submit them together with the written part, and make sure you upload the code to your Github repository.

\begin{problem}[1]
\textbf{(Murphy 11.3 - EM for Mixtures of Bernoullis)} Show that the M step for ML estimation
of a mixture of Bernoullis is given by
\[
    \mu_{kj} = \frac{\sum_i r_{ik}x_{ij}}{\sum_i r_{ik}}.
\]
Show that the M step for MAP estimation of a mixture of Bernoullis with a $\beta(a,b)$ prior
is given by
\[
    \mu_{kj} = \frac{\left(\sum_i r_{ik}x_{ij}\right) + a - 1}{\left(\sum_i r_{ik}\right) + a + b - 2}.
\]
\end{problem}
\begin{solution}
By definition, the log liklihood is defined as the following.

$$\ell(\mu) = \sum_{i} \sum_{k} r_{ik} \log P(x_i|\theta_k)
= \sum_{i} \sum_{k} r_{ik} \sum_{j} x_{ij}\log \mu_{kj} + (1 - x_{ij}) \log(1 - \mu_{kj})$$
Then, taking the derivitive with respect to $\mu_{kj}$, we get
$$
\frac{\partial \ell}{\partial \mu_{kj}} = \sum_{i} r_{ik} \left( \frac{x_{ij}}{\mu_{kj}} - \frac{1 - x_{ij}}{1 - \mu_{kj}} \right)
= \sum_{i} r_{ik} \frac{x_{ij} - \mu_{kj}}{\mu_{kj}(1 - \mu_{kj})}
= \frac{1}{\mu_{kj}(1 - \mu_{kj})} \sum_{i} r_{ik} (x_{ij} - \mu_{kj})
$$
This means that the optimality condition will be
$$
\sum_{i} r_{ik}x_{ij} = \mu_{kj} \sum_{i} r_{ik}
$$
as desired.

Next, for MAP estimation, assuming a $\beta(a,b)$ prior, we have our loss as

\begin{align*}
	\ell(\mu) &= \sum_{i} \sum_{k} r_{ik} \log P(x_i|\mu_k) + \log P(\mu_k)\\
	&= \sum_{i} \sum_{k} r_{ik} \left( \sum_{j} x_{ij} \log \mu_{kj} + (1 - x_{ij}) \log(1 - \mu_{kj}) \right) + (a - 1) \log \mu_{kj} + (b - 1) \log(1 - \mu_{kj})
\end{align*}
Taking the derivitive, we get
\begin{align*}
	\frac{\partial \ell}{\partial \mu} &= \sum_{i} \left( \frac{r_{ik}x_{ij} + a - 1}{\mu_{kj}} - \frac{r_{ik}(1 - x_{ij}) + b - 1}{1 - \mu_{kj}} \right)\\
&= - \frac{1}{\mu_{kj}(1 - \mu_{kj})} \sum_{i} \left( r_{ik}x_{ij} - r_{ik}\mu_{kj} + a - 1 - \mu_{kj}a + \mu_{kj} - \mu_{kj}b + \mu_{kj} \right)\\
&= - \frac{1}{\mu_{kj}(1 - \mu_{kj})} \left[ \left( \sum_{i} r_{ik}x_{ij} - \sum_{i} r_{ik} + a + b - 2 \right) \mu_{kj} + a - 1 \right]
\end{align*}
This means that the optimality condition will be
$$
\sum_{i} r_{ik}x_{ij} + a - 1 = \left( \sum_{i} r_{ik} + a + b - 2 \right) \mu_{kj}
$$
as desired.
\end{solution}
\newpage



\begin{problem}[2]
\textbf{(Lasso Feature Selection)} 
In this problem, we will use the online news popularity dataset we used in hw2pr3. In the starter code, we have already parsed the data for you. However, you might need internet connection to access the data and therefore successfully run the starter code.
\newline
\newline
First, ignoring undifferentiability at $x=0$, take $\frac{\partial |x|}{\partial x}
= \mathrm{sign} (x)$. Using this, show that $\nabla \|\xx\|_1 = \mathrm{sign}(\xx)$ where $\mathrm{sign}$ is applied
elementwise. Derive the gradient of the $\ell_1$ regularized linear regression objective
\begin{align*}
    \text{minimize: } & \|A\xx - \bb\|_2^2 + \lambda \|\xx\|_1
\end{align*}

Then, implement a gradient descent based solution of the above optimization problem for this data. Produce
the convergence plot (objective vs. iterations) for a non-trivial value of $\lambda$.
In the same figure (and different axes) produce a `regularization path' plot. Detailed
more in section 13.3.4 of Murphy, a regularization path is a plot of the optimal weight on
the $y$ axis at a given regularization strength $\lambda$ on the $x$ axis. Armed with this
plot, provide an ordered list of the top five features in predicting the log-shares of a news
article from this dataset (with justification).
\end{problem}
\begin{solution}
Let,
$$
\text{prox}_{\gamma}(x)_i = 
\begin{cases}
x_i - \gamma & x_i > \gamma \\
0 & |x_i| \leq \gamma \\
x_i + \gamma & x_i < -\gamma
\end{cases}
$$
Then, in each iteration, we have
$$
x_{i+1} = \text{prox}_{\gamma} (x_i - \gamma \nabla f(x_i))
$$
where \(\gamma\) is the learning rate.

Since \(\frac{\partial \|x\|_1}{\partial x_i} = \frac{\partial \sum |x_i|}{\partial x_i} = \text{sign}(x_i)\). It follows that \(\nabla \|x\|_1 = \text{sign}(x_i)\). Thus,
\begin{align*}
	\nabla \|Ax - b\|_2^2 + \lambda \|x\|_1 &= \nabla x^T A^T Ax - 2b^T Ax + b^T b + \lambda \|x\|_1\\
&= 2A^T Ax - 2b^T A + \lambda \text{sign}(x)
\end{align*}
which means that our objective is to
$$\text{minimize: } \|A\xx - \bb\|_2^2 + \lambda \|\xx\|_1.$$

Here is the convergence plot and regularization path:
\begin{center}
	\includegraphics{hw7pr2_lasso.png}
\end{center}


\end{solution}
\newpage

\end{document}
